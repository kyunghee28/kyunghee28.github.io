<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.11.2 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<html lang="ko" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>[Machine Learning] 인공신경망의 오버 피팅과 Regularization의 이해 - KYUNGHEE BLOG</title>
<meta name="description" content="오버 피팅과 learning curve, geometric explanation">



<meta property="og:type" content="website">
<meta property="og:locale" content="ko">
<meta property="og:site_name" content="KYUNGHEE BLOG">
<meta property="og:title" content="[Machine Learning] 인공신경망의 오버 피팅과 Regularization의 이해">
<meta property="og:url" content="http://localhost:4000/working/2019-02-01-overfit/">




  <meta property="og:image" content="http://localhost:4000/assets/images/top_1.jpg">









  

  


<link rel="canonical" href="http://localhost:4000/working/2019-02-01-overfit/">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "KIM KYUNGHEE",
      "url": "http://localhost:4000",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="KYUNGHEE BLOG Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">KYUNGHEE BLOG</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/" >Home</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/about/" >About</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/categories/" >Posts</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/tags/" >Tags</a>
            </li>
          
        </ul>
        
        <button class="search__toggle" type="button">
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">토글 메뉴</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      
  











<div class="page__hero--overlay"
  style=" background-image: url('/assets/images/top_1.jpg');"
>
  
    <div class="wrapper">
      <h1 id="page-title" class="page__title" itemprop="headline">
        
          [Machine Learning] 인공신경망의 오버 피팅과 Regularization의 이해

        
      </h1>
      
        <p class="page__lead">Overfitting and regularization of artificial neural netwroks
</p>
      
      
      
    </div>
  
  
</div>





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="http://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/assets/images/myimage.JPG" alt="김경희" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">김경희</h3>
    
    
      <p class="author__bio" itemprop="description">
        공부기록 블로그입니다.
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">팔로우</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="http://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">경기도</span>
        </li>
      

      

      
        <li>
          <a href="mailto:pori4339@gmail.com">
            <meta itemprop="email" content="pori4339@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> 이메일
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      
        <li>
          <a href="https://github.com/kyunghee28" itemprop="sameAs">
            <i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="[Machine Learning] 인공신경망의 오버 피팅과 Regularization의 이해">
    
    
    <meta itemprop="dateModified" content="February 01, 2019">

    <div class="page__inner-wrap">
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right ">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On This Page</h4></header>
              <ul class="toc__menu">
  <li><a href="#1-오버-피팅이란">1. 오버 피팅이란?</a></li>
  <li><a href="#2-오버-피팅이-발생하는-원인">2. 오버 피팅이 발생하는 원인</a></li>
</ul>
            </nav>
          </aside>
        
        <h1 id="overfitting-in-artificial-neural-networks">Overfitting in Artificial Neural Networks</h1>

<p>머신러닝, 인공신경망에서 필수적으로 알아야 할 개념 중 하나가 오버 피팅 (overfitting, 과적합)이다. 인터넷을 조금만 검색해보면 오버 피팅과 이를 방지하기위한 Regularization 방법에 관한 수 많은 포스트들이 있는데, 대부분 아래와 같은 두 가지 그림 중 하나를 사용하여 설명한다.</p>

<table>
  <thead>
    <tr>
      <th><img src="/assets/images/overfit1.png" alt="" /></th>
      <th><img src="/assets/images/overfit2.png" alt="" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>model complexity와 오버 피팅</em></td>
      <td><em>training epoch과 오버피팅</em></td>
    </tr>
  </tbody>
</table>

<p>다른 포스트들에서는 인공 신경망의 오버 피팅은 보통 우측의 그림과 함께 인공 신경망의 학습이 너무 <strong>과하게</strong> 이루어져서 발생한다는 설명과 함께 이를 방지하기 위한 early stopping (학습 조기 종료) 방법에 대해 설명한다.
또, 기계 학습 관점에서 보면 오버 피팅의 발생은 모델 복잡도에 따라 나타나는 좌측의 그림으로 설명하면서 L1/L2 regularization을 소개하기도한다.
하지만 유사한 두개의 그래프를 연결시켜 설명하는 글은 거의 본 적이 없다. (early stopping과 L1/L2 regularization도 마찬가지)
이 포스트는 오버 피팅과 L1/L2 regularization, early stopping에 대한 개인적인 해석을 담고 있다.</p>

<h2 id="1-오버-피팅이란">1. 오버 피팅이란?</h2>
<p>먼저 모델 학습을 위한 트레이닝 데이터를 $X_{train}$, 모델 테스트를 위한 테스트 데이터를 $X_{test}$라 한다. 각각의 데이터에 대한 모델의 오차를 계산할 수 있으므로 트레이닝 에러 $E_{train}$와 테스트 에러 $E_{test}$ 발생한다.</p>
<blockquote>
  <p>학습은 개인의 한정된 경험에 기반한다. 이는 기계 학습도 마찬가지다. <strong>트레이닝 데이터</strong> 는 우리가 모르는 원래의 어떤 모델로부터 생성된 데이터들을 <strong>샘플링한 일부</strong> 의 데이터에 해당한다. 따라서 <strong>트레이닝 에러</strong> 와 <strong>테스트 에러</strong> 를 각각 <code class="highlighter-rouge">in sample error, out of sample error</code> 라고도 한다.</p>
</blockquote>

<p>기계 학습의 목적은 결국 낮은 트레이닝 에러와 테스트 에러를 갖는 모델을 학습 하는 것이고, 학습이 진행됨에 따라 트레이닝 에러와 테스트 에러를 비교해보면 다음의 세가지 상황이 발생 할 수 있다.</p>

<ol>
  <li>$E_{train}$ : 높음, $E_{test}$ : 높음</li>
  <li>$E_{train}$ : 낮음, $E_{test}$ : 낮음</li>
  <li>$E_{train}$ : 매우 낮음, $E_{test}$ : 높음</li>
</ol>

<p>먼저 1번은 배운 문제도 잘 풀지 못하고 (high in sample error) 새로운 문제도 못푸는 (high out of sample error) 상황이다. 이러한 상황을 언더 피팅 (underfitting; 과소적합)이라한다. 이 경우 모델이 뭘 배웠다고 말할 수 있는 상황이 아니므 로 모델의 한계가 있는 것이거나 학습을 더 진행해야 하는 상황이다.
다음으로 2번의 경우가 배운 것도 잘하고 (low in sample error) 새 문제도 잘푸는 (low out of sample error), 학습이 잘 된 케이스이다. 우리는 기계 학습을 통해 이러한 결과를 갖는 모델을 얻기를 원한다.
마지막으로 3번은 배운 문제는 굉장히 잘 푸는데 (low in sample error) 오히려 새로운 문제는 못 푸는 (high out of sample error) 아이러니한 상황이고, 이를 오버 피팅 (overfitting; 과적합)이라 한다. 물론 테스트 데이터는 학습에 사용되지 않았기 때문에 $E_{test}\geq E_{train}$인 상황을 정상적으로 볼 수 있다. 하지만 오버 피팅은 $E_{train}$과 $E_{test}$의 차이가 정상 범위를 넘어서는 상황인것이다. 즉, 모델이 트레이닝 데이터를 과도하게 학습한 나머지 트레이닝 에러는 매우 낮지만 테스트 에러는 오히려 더 증가하는 상태이다. 따라서 오버 피팅된 모델은 일반화 성능을 나타내는 테스트 에러가 높기 때문에 실사용에 적합하지 않으며, 오버 피팅의 발생을 방지하는 것이 매우 중요하다.</p>

<blockquote>
  <p>오버피팅 : 모델의 트레이닝 에러가 매우 낮은데 비해 테스트 에러가 비정상적으로 높은 상태</p>
  <blockquote>
    <p>기출 문제는 100점 받는데 시험 점수는 6~70점 정도인 상황.</p>
  </blockquote>
</blockquote>

<h2 id="2-오버-피팅이-발생하는-원인">2. 오버 피팅이 발생하는 원인</h2>
<p>오버 피팅이 발생하는 원인은 무엇일까?</p>

<p>일반적으로 <strong>높은 모델 복잡도</strong> 를 원인으로 설명하는데, 그전에 오버 피팅 발생의 가장 근본적인 원인은 <strong>학습 데이터의 부족</strong> 이다.
극단적으로 학습한 데이터가 발생 할 수 있는 데이터의 전부라면 트레이닝 에러 뿐만아니라 테스트 에러도 낮을 것이 당연하다. 그러나 대부분의 상황에서 발생 가능한 모든 데이터를 수집하는것은 불가능하므로 우리는 <strong>한정된 데이터</strong> 를 통해 학습도 잘하고 새로운 문제도 잘 해결하는 모델을 찾아야 하는 것이다. 그 한정된 데이터가 어떤 모델을 설명하기에 충분히 많다면 오버 피팅은 발생하지 않는데, 충분하지 않은 경우 이제 <strong>모델 복잡도</strong> 와 오버 피팅의 발생이 연결되는 것이다.</p>

<blockquote>
  <p>오버 피팅 발생의 근본적인 원인은 학습 데이터의 부족이다.</p>
  <blockquote>
    <p>따라서 근본적인 해결책은 데이터를 충분히, 더 많이 모으는 것이다.</p>
  </blockquote>
</blockquote>

<p>하지만 우리는 학습하려는 실제 모델을 모르기때문에 어느정도의 데이터가 <strong>충분</strong> 한 것인지도 알 수 없다. 따라서 수집한 데이터 안에서 최대한 오버 피팅을 피하면서 성능이 좋은 모델을 개발해야 하는 한계가 있는 것이다.</p>

<p>이러한 상황에서 일반적인 오버 피팅의 설명은 데이터 대비 <strong>높은 모델 복잡도</strong> 를 갖는 것이 원인이 되어 발생하는 것으로 설명한다. 모델 복잡도란 간단하게 말해서 어떤 모델을 정의하기 위해 필요한 파라미터의 수로 이해할 수 있다. 즉 파라미터의 수가 많을수록 정교하고 복잡한 모델을 표현 할 수 있고, 적으면 간단한 모델만 설명할 수 있는 것이다. 모델 복잡도에 따라 발생하는 언더 피팅 오버 피팅의 그래프를 그려보면 다음과 같다.</p>

<table>
  <thead>
    <tr>
      <th><img src="/assets/images/overfit1.png" alt="" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>모델 복잡도와 오버 피팅 그래프</td>
    </tr>
  </tbody>
</table>

<p>그래프를 살펴보면 낮은 모델 복잡도에서는 트레이닝 에러와 테스트 에러 모두 높게 나오는데, 일정 수준까지는 모델 복잡도가 증가할수록 트레이닝 에러와 테스트 에러가 모두 감소하는 것을 볼 수 있다. 두 가지 에러가 모두 낮아지는 지점이 우리가 원하는 적절한 모델 복잡도를 갖는 모델일 것이다. 반면에 모델 복잡도가 일정 수준을 넘어가게 되면 트레이닝 에러는 지속적으로 감소하는데비해서 오히려 테스트 에러가 증가하게 된다. 이러한 상태가 오버 피팅이고 트레이닝 데이터를 통해 어떠한 관계를  <strong>학습</strong> 한것이 아니고 데이터 자체를 <strong>암기</strong> 해버린 것으로 설명하기도 한다.</p>

<p>오버 피팅과 언더 피팅에 대해 구체적인 예를 통해 살펴보자. 다음은 2차 함수 형태의 실제 모델 $f(x)$와 그 샘플링 데이터를 서로 다른 모델 복잡도를 갖는 다항 함수 $\hat{f}(x)$로 학습했을 때 그 결과 그래프이다. 검정색 실선과 점은 각각 실제 모델 $f(x)$와 그로부터 샘플링한 트레이닝 데이터를 나타낸다. 트레이닝 데이터로 학습한 $\hat{f}(x)$는 컬러 실선으로 표현되었다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/assets/images/overfit3.png" alt="" /></th>
      <th style="text-align: center"><img src="/assets/images/overfit4.png" alt="" /></th>
      <th style="text-align: center"><img src="/assets/images/overfit5.png" alt="" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">낮은 모델 복잡도</td>
      <td style="text-align: center">적정 모델 복잡도</td>
      <td style="text-align: center">높은 모델 복잡도</td>
    </tr>
    <tr>
      <td style="text-align: center">Underfitting</td>
      <td style="text-align: center">Moderate</td>
      <td style="text-align: center">Overfitting</td>
    </tr>
  </tbody>
</table>

<p>위의 그림에서 점들 (트레이닝 데이터)과 $\hat{f}(x)$와의 차이는 $E_{train}$, 검정 실선과 컬러 실선의 차이는 $E_{test}$를 의미한다. 첫번째는 2차 함수 형태의 실제 모델 $f(x)$를 모델 복잡도가 그보다 낮은 1차 함수로 모델링하는 상황으로 언더 피팅이 발생. 두번째의 경우 $\hat{f}(x)$ 또한 2차 함수이므로 트레이닝 / 테스트 에러가 모두 낮은 이상적인 상황이다. 마지막으로 $\hat{f}(x)$를 고차 다항식으로 설정했을때의 학습 결과인데, 샘플링한 모든 점과 가깝게 지나기때문에 트레이닝 에러는 매우 낮게 나오지만 $f(x)$와의 차이는 오히려 큰 폭으로 증가한 오버 피팅이 발생한 상황이다.</p>

<p>결과적으로 오버 피팅과 언더 피팅이 발생하지 않는 적절한 모델 복잡도를 갖도록 설계해야하는데 (<code class="highlighter-rouge">닭 잡는데 소 잡는 칼은 필요없다.</code>), 여기서 다시 학습하려는 실제 모델을 모른다는 문제점이 튀어나온다 (<code class="highlighter-rouge">하지만 뭘 잡게 될지 아직 모른는 상황!!</code>). 만약 알고있다면? 이러한 문제에서 고생할 이유가 전혀 없다. 기계 학습의 본질은 결국 실제 모델을 모르는 상황에서 경험적인 데이터를 통해서 자동적으로 모델을 학습, 유추하는 시스템이기 때문이다. 또한 적절한 모델 복잡도란 학습하려는 실제 모델에 따라 상대적인 것이므로 오버 피팅이란 꽤 까다로운 문제인 것이다.</p>

<p>여기서 한가지 짚고 넘어가야 할 포인트가 있다. 위의 그래프처럼 높은 모델 복잡도 (혹은 많은 모델 파라미터)를 갖는 모델을 학습시키면 항상 오버 피팅이 발생하는 것인가? 대부분의 오버 피팅 그래프에서 x축을 모델 복잡도로 표현하기때문에 간과하는 사실일수도 있는데, 모델 복잡도가 높다고해서 무조건 오버 피팅이 발생하는 것은 아니다.</p>

<p>모델 복잡도에 관한 오버 피팅 문제에서는, 모델 복잡도보다 모델의 표현력이라는 개념을 사용하는 것이 이해하기 쉽게 와닿는다. 모델의 표현력이란 해당 모델 복잡도를 갖는 모델의 표현 할 수 있는 한계를 의미한다. 따라서 모델의 표현력은 파라미터 수나 모델 복잡도가 고정적인 것으로 해석되는 것과는 차이가 있다.
좀 더 직관적으로 해석하기위해 색연필 세트의 예를 들 수 있다.</p>

<p>어떤 유명한 화가가 있다. 이 화가 지금까지 수십점의 작품을 공개했으며, 가장 큰 특징은 꼭 36색의 색연필 세트로 그림을 그린다는 것이다. 그리고 이 화가와 똑같이 그림을 그리고 싶은 3명의 학생 A, B, C가 있다. A, B, C 학생은 각각 12색, 36색, 100색 색연필 세트를 가지고 있으며, 화가가 발표한 작품들을 <strong>하나하나 디테일하게 반복적으로 따라그렸다.</strong> 화가가 새로운 작품을 발표했을 때 각 학생들은 그 작품을 얼마나 잘 따라 그릴 수 있을까?</p>

<p>위의 문제는 기계 학습의 각각의 과정에 대입할 수 있다. 따라 그리는데 사용한 공개된 수십점의 작품은 트레이닝 데이터, 따라 그리는 학생 A, B, C는 각각의 모델, 발표한 작품을 따라 그리는 과정은 학습, 그리고 새롭게 발표한 작품은 테스트 데이터에 해당한다. A, B, C가 모델에 해당하므로 각자가 가지고 있는 12색, 36색, 100색 색연필 세트는 곧 모델 파라미터 수이자 모델 복잡도이다.</p>

<p>이러한 상황에서 각 학생들은 화가의 화풍을 얼마나 잘 <strong>학습</strong> 했을까? 12색 색연필만 가지고 있는 A는 36색에 비해 가지고 있는 색이 턱없이 부족하기때문에 기존에 공개된 그림을 따라그리는 것은 물론 새로운 그림을 따라 그리는 것도 잘하지 못한다. 즉 언더 피팅이 발생한 것이다.
반면에 화가와 같이 36색의 색연필 세트를 가지고 있는 B 학생은 어떨까? 당연하겠지만 같은 구성의 색연필 세트를 사용했다는 것은 적절한 모델 복잡도의 선정에 성공했다는 뜻이므로 기존 그림과 새로운 그림 모두 잘 따라 그릴 수 있을것이다.
마지막으로 100색 색연필 세트를 가지고있는 C는 어떨까? 모든 학생 A, B, C는 최대한 디테일하게 반복적으로 기존 작품들을 따라 그림으로써 화풍을 학습했다. C학생은 100색의 색연필을 가지고 있기때문에, 너무 디테일하게 들어간 나머지, 그림의 흠집, 얼룩, 먼지 하나하나까지 따라 그린 것이다. 즉 기존 작품들을 얼룩까지 거의 복제하다싶이 그릴 수 있고, <strong>가장 큰 문제는 이를 얼룩이나 흠집이 아닌 화가의 화풍으로 이해해버린 것이다.</strong> 그렇기때문에 화가의 새로운 그림을 따라 그리는 과정에서 얼룩이나 흠집을 같이 표현하게 될 것이다. 즉 트레이닝 에러는 매우 낮은데 테스트 에러는 높은 오버 피팅이 발생한 것이다.</p>

<p>그런데 100색 색연필로 36색 색연필로 그린 화가의 그림을 잘 못따라 그린다는 것이 상식적으로 이해가 잘 되지 않는다. 단순하게 생각해서 100색 중에 36색만 사용하면 되는 문제가 아닐까? 표현력의 관점에서 보면 색상의 수는</p>

<p>먼저 우리의 목적은 어떤 화가의 화풍을 똑같이 따르는 그림 그리는 법을 배우는 것이다.
12색, 36색, 100색 색연필 세트가 있다고 하자. 색연필 세트가 보유하고 있는 색의 종류는 곧 모델 파라미터 수이자 모델 복잡도를 의미한다. 모델 표현력은 해당 컬러 구성으로 표현 할 수 있는 그림의 한계로 해석 할 수 있다. 그리고 우리에게는 트레이닝 데이터로서 36색 색연필로 그려진 여러장의 그림이 있다. 이 그림들을 각각의 색연필 세트로 <strong>따라 그리는 과정</strong> 을 통해 36색 그림을 학습하는 것이다. 그리고 학습이 끝나면 배운 그림들로
우리의 목적이 36색 색연필로 그려진 그림들을 따라 그리는 것이라고 하면, 12색 색연필 만으로는 따라 그리는데 한계가 있다. 즉 언더 피팅이 발생한다. 36색 색연필 세트로는 동일한 색연필 세트를 사용한 그림이기때문에 충분하게 잘 그릴 수 있다. 100색 색연필 세트를 사용하는 경우는 어떨까?? 모델 복잡도의 개념으로 접근한다면 100색 색연필 세트로 그렸을때는 오버 피팅이 발생할 것 처럼 보인다. 즉 100색 색연필은 굉장히 다양한 색을 표현할 수 있기때문에 그림이 아닌 캔버스에 있는 흠집이나, 얼룩, 혹은 먼지와 같은 사소한 것까지 따라 그리는 것이 곧 오버 피팅인 것이다.
그런데 100색 색연필 세트로 36색으로 그린 그림을
하지만 표현력의 관점에서보면 100색 색연필 세트로 36색 그림들을 표현할 수 있는 것이 당연하다. 간단하게 100색 중에 그림에 사용된 36색만 사용하면 되는 것이기 때문이다. 따라서 100색 색연필은 100색 이하의 색 구성을 가진 그림들은 모두 표현 가능한 것이다.</p>

<p>모델 혹은 함수의 복잡한 정도  즉 높은 모델 복잡도 = 많은 모델 파라미터 = 높은 모델의 표현력의 관계가 성립한다.<br />
따라서 낮은 모델 복잡도를 사용하는 모델은 학습 자체가 어렵기때문에 모델 복잡도가 높은 모델을 사용하고 오버피팅이 발생하지 않도록 제한하는 방향으로 접근하게된다.</p>

<ol>
  <li>학습 데이터의 부족</li>
  <li>데이터 대비 높은 모델 복잡도</li>
</ol>

        
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <div align="center" style="margin: 1em 0;">
        <ins class="adsbygoogle"
             style="display:block; border-bottom: initial;"
             data-ad-client="ca-pub-9089895411733030"
             data-ad-format="auto"></ins>
        </div>
        <script>
        (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      </section>

      <footer class="page__meta">
        
        


  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> 카테고리: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/categories/#machine-learning" class="page__taxonomy-item" rel="tag">Machine Learning</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 업데이트:</strong> <time datetime="2019-02-01">February 01, 2019</time></p>
        

      </footer>

      

      
    </div>

    
  </article>

  
  
</div>

    </div>

    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <div align="center" style="margin: 1em 0;">
    <ins class="adsbygoogle"
         style="display:block; border-bottom: initial;"
         data-ad-client="ca-pub-9089895411733030"
         data-ad-format="auto"></ins>
    </div>
    <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
    </script>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><input type="text" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
    <div id="results" class="results"></div></div>
      </div>
    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->

        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>팔로우:</strong></li>
    
    
    
    
      <li><a href="https://github.com/kyunghee28"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    
    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> 피드</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2019 KIM KYUNGHEE. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.<br /><br />
<a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="크리에이티브 커먼즈 라이선스" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a><br /><br />이 저작물은 <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/">크리에이티브 커먼즈 저작자표시-비영리-변경금지 4.0 국제 라이선스</a>에 따라 이용할 수 있습니다. </div>

      </footer>
    </div>

    
<script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.13/js/all.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    
  <script>
    var disqus_config = function () {
      this.page.url = "http://localhost:4000/working/2019-02-01-overfit/";  // Replace PAGE_URL with your page's canonical URL variable
      this.page.identifier = ""; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    (function() { // DON'T EDIT BELOW THIS LINE
      var d = document, s = d.createElement('script');
      s.src = 'https://kyunhgee28.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  



  </body>
</html>
